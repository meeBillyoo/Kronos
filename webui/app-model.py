#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Kronos æ¨¡å‹ç®¡ç†æ¨¡å—
æä¾›æ¨¡å‹åŠ è½½ã€é¢„æµ‹å’ŒçŠ¶æ€ç®¡ç†åŠŸèƒ½

ä¸»è¦åŠŸèƒ½:
- æ¨¡å‹é…ç½®å’ŒåŠ è½½ç®¡ç†
- æ•°æ®æ–‡ä»¶å¤„ç†å’ŒéªŒè¯
- é¢„æµ‹ç»“æœä¿å­˜å’Œåˆ†æ
- æ¨¡å‹çŠ¶æ€ç›‘æ§

ä½œè€…: Kronos Team
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¶é—´: 2024
"""

import os
import pandas as pd
import numpy as np
import json
import sys
import warnings
import datetime
from typing import Optional, Dict, Any, Tuple, List

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å‹æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°è¯•å¯¼å…¥Kronosæ¨¡å‹ç›¸å…³ç±»
try:
    from model import Kronos, KronosTokenizer, KronosPredictor
    MODEL_AVAILABLE = True
except ImportError:
    MODEL_AVAILABLE = False
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥ Kronos æ¨¡å‹ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º")

# å…¨å±€å˜é‡å­˜å‚¨å·²åŠ è½½çš„æ¨¡å‹ç»„ä»¶
tokenizer = None  # åˆ†è¯å™¨å®ä¾‹
model = None      # æ¨¡å‹å®ä¾‹
predictor = None  # é¢„æµ‹å™¨å®ä¾‹

# è®¡ç®—æ¨¡å‹ç›®å½•çš„ç»å¯¹è·¯å¾„
MODEL_BASE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model')

# å¯ç”¨æ¨¡å‹é…ç½®å­—å…¸
# åŒ…å«ä¸åŒè§„æ¨¡çš„Kronosæ¨¡å‹é…ç½®ä¿¡æ¯
AVAILABLE_MODELS = {
    'kronos-mini': {
        'name': 'Kronos-mini',
        'model_id': 'NeoQuasar/Kronos-mini',
        'tokenizer_id': 'NeoQuasar/Kronos-Tokenizer-2k',
        'context_length': 2048,
        'params': '4.1M',
        'description': 'è½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆå¿«é€Ÿé¢„æµ‹ï¼Œå‚æ•°é‡å°ï¼Œæ¨ç†é€Ÿåº¦å¿«'
    },
    'kronos-small': {
        'name': 'Kronos-small',
        'model_id': 'NeoQuasar/Kronos-small',
        'tokenizer_id': 'NeoQuasar/Kronos-Tokenizer-base',
        'context_length': 512,
        'params': '24.7M',
        'description': 'å°å‹æ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦ï¼Œé€‚åˆä¸€èˆ¬é¢„æµ‹ä»»åŠ¡'
    },
    'kronos-base': {
        'name': 'Kronos-base',
        'model_id': 'NeoQuasar/Kronos-base',
        'tokenizer_id': 'NeoQuasar/Kronos-Tokenizer-base',
        'context_length': 512,
        'params': '102.3M',
        'description': 'åŸºç¡€æ¨¡å‹ï¼Œæä¾›æ›´å¥½çš„é¢„æµ‹è´¨é‡ï¼Œé€‚åˆé«˜ç²¾åº¦é¢„æµ‹éœ€æ±‚'
    }
}


def load_data_files() -> List[Dict[str, str]]:
    """
    æ‰«ææ•°æ®ç›®å½•å¹¶è¿”å›å¯ç”¨çš„æ•°æ®æ–‡ä»¶åˆ—è¡¨
    
    åŠŸèƒ½è¯´æ˜:
    - æ‰«æwebui/dataç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
    - æ”¯æŒCSVå’ŒFeatheræ ¼å¼æ–‡ä»¶
    - è®¡ç®—æ–‡ä»¶å¤§å°å¹¶æ ¼å¼åŒ–æ˜¾ç¤º
    
    Returns:
        List[Dict[str, str]]: åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
                             æ¯ä¸ªå­—å…¸åŒ…å«nameã€pathã€sizeå­—æ®µ
    """
    # æ„å»ºæ•°æ®ç›®å½•è·¯å¾„
    data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')
    data_files = []
    
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if os.path.exists(data_dir):
        # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for file in os.listdir(data_dir):
            # åªå¤„ç†æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
            if file.endswith(('.csv', '.feather')):
                file_path = os.path.join(data_dir, file)
                file_size = os.path.getsize(file_path)
                
                # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º
                size_str = (
                    f"{file_size / 1024:.1f} KB" if file_size < 1024*1024 
                    else f"{file_size / (1024*1024):.1f} MB"
                )
                
                data_files.append({
                    'name': file,
                    'path': file_path,
                    'size': size_str
                })
    
    return data_files


def load_data_file(file_path: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """
    åŠ è½½æ•°æ®æ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†
    
    åŠŸèƒ½è¯´æ˜:
    - æ”¯æŒCSVå’ŒFeatheræ ¼å¼æ–‡ä»¶åŠ è½½
    - éªŒè¯å¿…éœ€çš„OHLCåˆ—æ˜¯å¦å­˜åœ¨
    - å¤„ç†æ—¶é—´æˆ³åˆ—çš„å¤šç§æ ¼å¼
    - æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…ç†
    
    Args:
        file_path (str): æ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        
    Returns:
        Tuple[Optional[pd.DataFrame], Optional[str]]: 
            - æˆåŠŸæ—¶è¿”å›(DataFrame, None)
            - å¤±è´¥æ—¶è¿”å›(None, é”™è¯¯ä¿¡æ¯)
    """
    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹æ³•
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith('.feather'):
            df = pd.read_feather(file_path)
        else:
            return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒCSVå’ŒFeatheræ ¼å¼"
        
        # æ£€æŸ¥å¿…éœ€çš„OHLCåˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ['open', 'high', 'low', 'close']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            return None, f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}"
        
        # å¤„ç†æ—¶é—´æˆ³åˆ—ï¼Œæ”¯æŒå¤šç§åˆ—åæ ¼å¼
        timestamp_cols = ['timestamps', 'timestamp', 'date', 'time']
        timestamp_col = None
        
        for col in timestamp_cols:
            if col in df.columns:
                timestamp_col = col
                break
        
        if timestamp_col:
            # è½¬æ¢ç°æœ‰çš„æ—¶é—´æˆ³åˆ—
            df['timestamps'] = pd.to_datetime(df[timestamp_col])
            # å¦‚æœåŸåˆ—åä¸æ˜¯timestampsï¼Œåˆ é™¤åŸåˆ—
            if timestamp_col != 'timestamps':
                df = df.drop(columns=[timestamp_col])
        else:
            # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³åˆ—ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æ—¶é—´åºåˆ—
            print("è­¦å‘Š: æœªæ‰¾åˆ°æ—¶é—´æˆ³åˆ—ï¼Œå°†åˆ›å»ºé»˜è®¤æ—¶é—´åºåˆ—")
            df['timestamps'] = pd.date_range(
                start='2024-01-01', 
                periods=len(df), 
                freq='1H'
            )
        
        # ç¡®ä¿OHLCåˆ—æ˜¯æ•°å€¼ç±»å‹
        for col in required_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # å¤„ç†å¯é€‰çš„æˆäº¤é‡åˆ—
        if 'volume' in df.columns:
            df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        
        # å¤„ç†å¯é€‰çš„æˆäº¤é¢åˆ—ï¼ˆé€šå¸¸ä¸ç”¨äºé¢„æµ‹ï¼‰
        if 'amount' in df.columns:
            df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
        
        # åˆ é™¤åŒ…å«NaNå€¼çš„è¡Œ
        original_len = len(df)
        df = df.dropna()
        
        if len(df) < original_len:
            print(f"è­¦å‘Š: åˆ é™¤äº† {original_len - len(df)} è¡ŒåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®")
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        df = df.sort_values('timestamps').reset_index(drop=True)
        
        return df, None
        
    except Exception as e:
        return None, f"åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}"


def save_prediction_results(file_path: str, prediction_type: str, prediction_results: List[Dict], 
                          actual_data: List[Dict], input_data: pd.DataFrame, 
                          prediction_params: Dict[str, Any]) -> Optional[str]:
    """
    ä¿å­˜é¢„æµ‹ç»“æœåˆ°JSONæ–‡ä»¶
    
    åŠŸèƒ½è¯´æ˜:
    - åˆ›å»ºé¢„æµ‹ç»“æœç›®å½•
    - ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    - ä¿å­˜å®Œæ•´çš„é¢„æµ‹ä¿¡æ¯å’Œåˆ†æç»“æœ
    - è®¡ç®—é¢„æµ‹ä¸å®é™…æ•°æ®çš„å·®å¼‚åˆ†æ
    
    Args:
        file_path (str): åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
        prediction_type (str): é¢„æµ‹ç±»å‹æ ‡è¯†
        prediction_results (List[Dict]): é¢„æµ‹ç»“æœåˆ—è¡¨
        actual_data (List[Dict]): å®é™…æ•°æ®åˆ—è¡¨ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        input_data (pd.DataFrame): è¾“å…¥æ•°æ®
        prediction_params (Dict[str, Any]): é¢„æµ‹å‚æ•°
        
    Returns:
        Optional[str]: æˆåŠŸæ—¶è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # åˆ›å»ºé¢„æµ‹ç»“æœå­˜å‚¨ç›®å½•
        results_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'prediction_results')
        os.makedirs(results_dir, exist_ok=True)
        
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'prediction_{timestamp}.json'
        filepath = os.path.join(results_dir, filename)
        
        # æ„å»ºä¿å­˜æ•°æ®ç»“æ„
        save_data = {
            'metadata': {
                'timestamp': datetime.datetime.now().isoformat(),
                'file_path': file_path,
                'prediction_type': prediction_type,
                'prediction_params': prediction_params
            },
            'input_data_summary': {
                'total_rows': len(input_data),
                'columns': list(input_data.columns),
                'time_range': {
                    'start': input_data['timestamps'].min().isoformat() if 'timestamps' in input_data.columns else None,
                    'end': input_data['timestamps'].max().isoformat() if 'timestamps' in input_data.columns else None
                },
                'price_statistics': {
                    'open': {
                        'min': float(input_data['open'].min()), 
                        'max': float(input_data['open'].max()),
                        'mean': float(input_data['open'].mean())
                    },
                    'high': {
                        'min': float(input_data['high'].min()), 
                        'max': float(input_data['high'].max()),
                        'mean': float(input_data['high'].mean())
                    },
                    'low': {
                        'min': float(input_data['low'].min()), 
                        'max': float(input_data['low'].max()),
                        'mean': float(input_data['low'].mean())
                    },
                    'close': {
                        'min': float(input_data['close'].min()), 
                        'max': float(input_data['close'].max()),
                        'mean': float(input_data['close'].mean())
                    }
                },
                'last_values': {
                    'open': float(input_data['open'].iloc[-1]),
                    'high': float(input_data['high'].iloc[-1]),
                    'low': float(input_data['low'].iloc[-1]),
                    'close': float(input_data['close'].iloc[-1])
                }
            },
            'prediction_results': prediction_results,
            'actual_data': actual_data,
            'analysis': {}
        }
        
        # å¦‚æœå­˜åœ¨å®é™…æ•°æ®ï¼Œè¿›è¡Œé¢„æµ‹å‡†ç¡®æ€§åˆ†æ
        if actual_data and len(actual_data) > 0 and len(prediction_results) > 0:
            # è®¡ç®—é¢„æµ‹ä¸å®é™…æ•°æ®çš„è¿ç»­æ€§åˆ†æ
            last_pred = prediction_results[0]  # ç¬¬ä¸€ä¸ªé¢„æµ‹ç‚¹
            first_actual = actual_data[0]      # ç¬¬ä¸€ä¸ªå®é™…ç‚¹
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„ç»å¯¹å·®å¼‚å’Œç›¸å¯¹å·®å¼‚
            gaps = {}
            gap_percentages = {}
            
            for metric in ['open', 'high', 'low', 'close']:
                if metric in last_pred and metric in first_actual:
                    abs_gap = abs(last_pred[metric] - first_actual[metric])
                    rel_gap = (abs_gap / first_actual[metric]) * 100 if first_actual[metric] != 0 else 0
                    
                    gaps[f'{metric}_gap'] = abs_gap
                    gap_percentages[f'{metric}_gap_pct'] = rel_gap
            
            save_data['analysis']['continuity'] = {
                'description': 'é¢„æµ‹å€¼ä¸å®é™…å€¼çš„è¿ç»­æ€§åˆ†æ',
                'last_prediction': last_pred,
                'first_actual': first_actual,
                'absolute_gaps': gaps,
                'relative_gaps_percent': gap_percentages,
                'overall_accuracy': {
                    'avg_relative_error': np.mean(list(gap_percentages.values())),
                    'max_relative_error': max(gap_percentages.values()) if gap_percentages else 0
                }
            }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath
        
    except Exception as e:
        print(f"ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {e}")
        return None


def load_model(model_key: str, device: str = 'cpu') -> Dict[str, Any]:
    """
    åŠ è½½æŒ‡å®šçš„Kronosæ¨¡å‹
    
    åŠŸèƒ½è¯´æ˜:
    - éªŒè¯æ¨¡å‹å¯ç”¨æ€§å’Œæ¨¡å‹é”®çš„æœ‰æ•ˆæ€§
    - åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
    - åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
    - æ›´æ–°å…¨å±€æ¨¡å‹çŠ¶æ€
    
    Args:
        model_key (str): æ¨¡å‹é”®åï¼Œå¿…é¡»åœ¨AVAILABLE_MODELSä¸­
        device (str): è¿è¡Œè®¾å¤‡ï¼Œé»˜è®¤ä¸º'cpu'ï¼Œå¯é€‰'cuda'
        
    Returns:
        Dict[str, Any]: åŒ…å«åŠ è½½ç»“æœçš„å­—å…¸
                       æˆåŠŸæ—¶code=0ï¼Œå¤±è´¥æ—¶code!=0
    """
    global tokenizer, model, predictor
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        if not MODEL_AVAILABLE:
            return {
                'code': 500,
                'message': 'æ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…',
                'data': None
            }
        
        # éªŒè¯æ¨¡å‹é”®æ˜¯å¦æœ‰æ•ˆ
        if model_key not in AVAILABLE_MODELS:
            available_keys = list(AVAILABLE_MODELS.keys())
            return {
                'code': 400,
                'message': f'ä¸æ”¯æŒçš„æ¨¡å‹: {model_key}ï¼Œå¯ç”¨æ¨¡å‹: {available_keys}',
                'data': None
            }
        
        # è·å–æ¨¡å‹é…ç½®
        model_config = AVAILABLE_MODELS[model_key]
        print(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_config['name']}...")
        
        # åŠ è½½åˆ†è¯å™¨
        print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        tokenizer = KronosTokenizer.from_pretrained(model_config['tokenizer_id'])
        
        # åŠ è½½æ¨¡å‹
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model = Kronos.from_pretrained(model_config['model_id'])
        
        # åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
        print("æ­£åœ¨åˆ›å»ºé¢„æµ‹å™¨...")
        predictor = KronosPredictor(
            model, 
            tokenizer, 
            device=device, 
            max_context=model_config['context_length']
        )
        
        success_msg = f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_config['name']} ({model_config['params']}) åœ¨ {device} è®¾å¤‡ä¸Š"
        print(success_msg)
        
        return {
            'code': 0,
            'message': success_msg,
            'data': {
                'success': True,
                'model_info': {
                    'key': model_key,
                    'name': model_config['name'],
                    'params': model_config['params'],
                    'context_length': model_config['context_length'],
                    'description': model_config['description'],
                    'device': device
                }
            }
        }
        
    except Exception as e:
        error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        print(error_msg)
        return {
            'code': 500,
            'message': error_msg,
            'data': None
        }


def get_available_models() -> Dict[str, Any]:
    """
    è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„é…ç½®ä¿¡æ¯
    
    Returns:
        Dict[str, Any]: åŒ…å«æ‰€æœ‰å¯ç”¨æ¨¡å‹ä¿¡æ¯çš„å­—å…¸
    """
    try:
        return {
            'code': 0,
            'message': 'è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨æˆåŠŸ',
            'data': {
                'models': AVAILABLE_MODELS,
                'model_available': MODEL_AVAILABLE,
                'total_models': len(AVAILABLE_MODELS)
            }
        }
    except Exception as e:
        return {
            'code': 500,
            'message': f'è·å–å¯ç”¨æ¨¡å‹å¤±è´¥: {str(e)}',
            'data': None
        }


def get_model_status() -> Dict[str, Any]:
    """
    è·å–å½“å‰æ¨¡å‹çš„åŠ è½½çŠ¶æ€
    
    åŠŸèƒ½è¯´æ˜:
    - æ£€æŸ¥æ¨¡å‹ç»„ä»¶çš„åŠ è½½çŠ¶æ€
    - è¿”å›å½“å‰æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
    - æä¾›è®¾å¤‡å’Œä¸Šä¸‹æ–‡é•¿åº¦ä¿¡æ¯
    
    Returns:
        Dict[str, Any]: åŒ…å«æ¨¡å‹çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
    """
    try:
        if MODEL_AVAILABLE:
            # æ„å»ºè¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
            status = {
                'model_available': True,
                'model_loaded': model is not None,
                'tokenizer_loaded': tokenizer is not None,
                'predictor_ready': predictor is not None,
                'components_status': {
                    'tokenizer': 'loaded' if tokenizer is not None else 'not_loaded',
                    'model': 'loaded' if model is not None else 'not_loaded',
                    'predictor': 'ready' if predictor is not None else 'not_ready'
                }
            }
            
            # å¦‚æœé¢„æµ‹å™¨å·²å°±ç»ªï¼Œæ·»åŠ è¯¦ç»†ä¿¡æ¯
            if predictor is not None:
                status['current_model'] = {
                    'device': str(predictor.device),
                    'max_context': predictor.max_context,
                    'model_type': type(predictor.model).__name__,
                    'tokenizer_type': type(predictor.tokenizer).__name__
                }
        else:
            # æ¨¡å‹ä¸å¯ç”¨æ—¶çš„çŠ¶æ€
            status = {
                'model_available': False,
                'model_loaded': False,
                'tokenizer_loaded': False,
                'predictor_ready': False,
                'error_reason': 'æ¨¡å‹æ¨¡å—å¯¼å…¥å¤±è´¥'
            }
        
        return {
            'code': 0,
            'message': 'è·å–æ¨¡å‹çŠ¶æ€æˆåŠŸ',
            'data': status
        }
        
    except Exception as e:
        return {
            'code': 500,
            'message': f'è·å–æ¨¡å‹çŠ¶æ€å¤±è´¥: {str(e)}',
            'data': None
        }


def get_model_instance() -> Dict[str, Any]:
    """
    è·å–æ¨¡å‹å®ä¾‹çš„çŠ¶æ€ä¿¡æ¯ï¼ˆç”¨äºå†…éƒ¨è°ƒç”¨ï¼‰
    
    Returns:
        Dict[str, Any]: æ¨¡å‹å®ä¾‹çŠ¶æ€å­—å…¸
    """
    status = {
        'model_available': MODEL_AVAILABLE,
        'model_loaded': model,
        'tokenizer_loaded': tokenizer,
        'predictor_ready': predictor
    }
    
    # å¦‚æœé¢„æµ‹å™¨å­˜åœ¨ï¼Œæ·»åŠ å½“å‰æ¨¡å‹ä¿¡æ¯
    if predictor is not None:
        status['current_model'] = {
            'device': str(predictor.device),
            'max_context': predictor.max_context
        }
    
    return status


def test_data_loading():
    """
    æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½
    
    æµ‹è¯•å†…å®¹:
    - æ‰«ææ•°æ®ç›®å½•
    - åŠ è½½ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ•°æ®æ–‡ä»¶
    - éªŒè¯æ•°æ®æ ¼å¼å’Œå†…å®¹
    """
    print("\n" + "="*50)
    print("æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½")
    print("="*50)
    
    # æµ‹è¯•æ•°æ®æ–‡ä»¶æ‰«æ
    print("\n1. æ‰«ææ•°æ®æ–‡ä»¶...")
    data_files = load_data_files()
    print(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
    
    if not data_files:
        print("  âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
        print("  ğŸ’¡ è¯·åœ¨ webui/data/ ç›®å½•ä¸‹æ”¾ç½®CSVæˆ–Featheræ ¼å¼çš„æ•°æ®æ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ–‡ä»¶
    for i, file_info in enumerate(data_files, 1):
        print(f"  {i}. {file_info['name']} ({file_info['size']})")
    
    # æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶
    print("\n2. æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶...")
    first_file = data_files[0]['path']
    print(f"æ­£åœ¨åŠ è½½: {os.path.basename(first_file)}")
    
    df, error = load_data_file(first_file)
    if error:
        print(f"  âŒ åŠ è½½å¤±è´¥: {error}")
    else:
        print(f"  âœ… åŠ è½½æˆåŠŸ!")
        print(f"  ğŸ“Š æ•°æ®è¡Œæ•°: {len(df)}")
        print(f"  ğŸ“‹ åˆ—å: {list(df.columns)}")
        
        if len(df) > 0:
            print(f"  ğŸ“… æ—¶é—´èŒƒå›´: {df['timestamps'].min()} åˆ° {df['timestamps'].max()}")
            print(f"  ğŸ’° ä»·æ ¼èŒƒå›´: {df['close'].min():.2f} - {df['close'].max():.2f}")
            print(f"  ğŸ“ˆ æœ€åä¸€è¡Œæ•°æ®:")
            last_row = df.iloc[-1]
            for col in ['timestamps', 'open', 'high', 'low', 'close']:
                if col in last_row:
                    value = last_row[col]
                    if col == 'timestamps':
                        print(f"    {col}: {value}")
                    else:
                        print(f"    {col}: {value:.4f}")


def test_model_functions():
    """
    æµ‹è¯•æ¨¡å‹ç›¸å…³åŠŸèƒ½
    
    æµ‹è¯•å†…å®¹:
    - è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    - æ£€æŸ¥æ¨¡å‹çŠ¶æ€
    - å°è¯•åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    print("\n" + "="*50)
    print("æµ‹è¯•æ¨¡å‹åŠŸèƒ½")
    print("="*50)
    
    # æµ‹è¯•è·å–å¯ç”¨æ¨¡å‹
    print("\n1. è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
    models_result = get_available_models()
    
    if models_result['code'] == 0:
        print(f"  âœ… {models_result['message']}")
        data = models_result['data']
        print(f"  ğŸ¤– æ¨¡å‹å¯ç”¨æ€§: {'âœ… å¯ç”¨' if data['model_available'] else 'âŒ ä¸å¯ç”¨'}")
        print(f"  ğŸ“¦ æ¨¡å‹æ•°é‡: {data['total_models']}")
        
        print("\n  å¯ç”¨æ¨¡å‹è¯¦æƒ…:")
        for key, model_info in data['models'].items():
            print(f"    ğŸ”¹ {key}:")
            print(f"      åç§°: {model_info['name']}")
            print(f"      å‚æ•°: {model_info['params']}")
            print(f"      ä¸Šä¸‹æ–‡: {model_info['context_length']}")
            print(f"      æè¿°: {model_info['description']}")
    else:
        print(f"  âŒ {models_result['message']}")
    
    # æµ‹è¯•è·å–æ¨¡å‹çŠ¶æ€
    print("\n2. æ£€æŸ¥æ¨¡å‹çŠ¶æ€...")
    status_result = get_model_status()
    
    if status_result['code'] == 0:
        print(f"  âœ… {status_result['message']}")
        status = status_result['data']
        print(f"  ğŸ”§ æ¨¡å‹å¯ç”¨: {'âœ…' if status['model_available'] else 'âŒ'}")
        print(f"  ğŸ“¦ æ¨¡å‹å·²åŠ è½½: {'âœ…' if status['model_loaded'] else 'âŒ'}")
        print(f"  ğŸ”¤ åˆ†è¯å™¨å·²åŠ è½½: {'âœ…' if status['tokenizer_loaded'] else 'âŒ'}")
        print(f"  ğŸš€ é¢„æµ‹å™¨å°±ç»ª: {'âœ…' if status['predictor_ready'] else 'âŒ'}")
        
        if 'current_model' in status:
            current = status['current_model']
            print(f"  ğŸ’» å½“å‰è®¾å¤‡: {current['device']}")
            print(f"  ğŸ“ æœ€å¤§ä¸Šä¸‹æ–‡: {current['max_context']}")
    else:
        print(f"  âŒ {status_result['message']}")
    
    # å¦‚æœæ¨¡å‹å¯ç”¨ï¼Œæµ‹è¯•åŠ è½½
    if MODEL_AVAILABLE:
        print("\n3. æµ‹è¯•æ¨¡å‹åŠ è½½...")
        print("  æ­£åœ¨å°è¯•åŠ è½½ kronos-mini æ¨¡å‹...")
        
        load_result = load_model('kronos-mini', 'cpu')
        if load_result['code'] == 0:
            print(f"  âœ… {load_result['message']}")
            model_info = load_result['data']['model_info']
            print(f"  ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
            print(f"    åç§°: {model_info['name']}")
            print(f"    å‚æ•°é‡: {model_info['params']}")
            print(f"    è®¾å¤‡: {model_info['device']}")
            print(f"    ä¸Šä¸‹æ–‡é•¿åº¦: {model_info['context_length']}")
        else:
            print(f"  âŒ {load_result['message']}")
    else:
        print("\n3. è·³è¿‡æ¨¡å‹åŠ è½½æµ‹è¯•")
        print("  âš ï¸  æ¨¡å‹ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒåŠ è½½æµ‹è¯•")
        print("  ğŸ’¡ è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…Kronosæ¨¡å‹æ¨¡å—")


def test_utility_functions():
    """
    æµ‹è¯•å·¥å…·å‡½æ•°
    
    æµ‹è¯•å†…å®¹:
    - æ¨¡å‹å®ä¾‹çŠ¶æ€è·å–
    - é¢„æµ‹ç»“æœä¿å­˜åŠŸèƒ½
    """
    print("\n" + "="*50)
    print("æµ‹è¯•å·¥å…·å‡½æ•°")
    print("="*50)
    
    # æµ‹è¯•æ¨¡å‹å®ä¾‹çŠ¶æ€è·å–
    print("\n1. æµ‹è¯•æ¨¡å‹å®ä¾‹çŠ¶æ€è·å–...")
    instance_status = get_model_instance()
    print(f"  ğŸ“Š æ¨¡å‹å®ä¾‹çŠ¶æ€:")
    print(f"    æ¨¡å‹å¯ç”¨: {'âœ…' if instance_status['model_available'] else 'âŒ'}")
    print(f"    æ¨¡å‹å·²åŠ è½½: {'âœ…' if instance_status['model_loaded'] is not None else 'âŒ'}")
    print(f"    åˆ†è¯å™¨å·²åŠ è½½: {'âœ…' if instance_status['tokenizer_loaded'] is not None else 'âŒ'}")
    print(f"    é¢„æµ‹å™¨å°±ç»ª: {'âœ…' if instance_status['predictor_ready'] is not None else 'âŒ'}")
    
    if 'current_model' in instance_status:
        current = instance_status['current_model']
        print(f"    å½“å‰è®¾å¤‡: {current['device']}")
        print(f"    æœ€å¤§ä¸Šä¸‹æ–‡: {current['max_context']}")
    
    # æµ‹è¯•é¢„æµ‹ç»“æœä¿å­˜åŠŸèƒ½
    print("\n2. æµ‹è¯•é¢„æµ‹ç»“æœä¿å­˜åŠŸèƒ½...")
    print("  åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    mock_input_data = pd.DataFrame({
        'open': [100.0, 101.0, 102.0],
        'high': [105.0, 106.0, 107.0],
        'low': [95.0, 96.0, 97.0],
        'close': [103.0, 104.0, 105.0],
        'volume': [1000, 1100, 1200],
        'timestamps': pd.date_range('2024-01-01', periods=3, freq='1H')
    })
    
    # åˆ›å»ºæ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    mock_prediction = [
        {
            'open': 106.0, 'high': 108.0, 'low': 104.0, 'close': 107.0, 
            'timestamp': '2024-01-01T04:00:00'
        },
        {
            'open': 107.0, 'high': 109.0, 'low': 105.0, 'close': 108.0, 
            'timestamp': '2024-01-01T05:00:00'
        }
    ]
    
    # åˆ›å»ºæ¨¡æ‹Ÿå®é™…æ•°æ®ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    mock_actual = [
        {
            'open': 105.5, 'high': 107.5, 'low': 103.5, 'close': 106.5, 
            'timestamp': '2024-01-01T04:00:00'
        }
    ]
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    saved_path = save_prediction_results(
        file_path='test_data.csv',
        prediction_type='unit_test',
        prediction_results=mock_prediction,
        actual_data=mock_actual,
        input_data=mock_input_data,
        prediction_params={
            'model': 'kronos-mini',
            'steps': 2,
            'device': 'cpu',
            'test_mode': True
        }
    )
    
    if saved_path:
        print(f"  âœ… é¢„æµ‹ç»“æœä¿å­˜æˆåŠŸ")
        print(f"  ğŸ“ ä¿å­˜è·¯å¾„: {saved_path}")
        print(f"  ğŸ“„ æ–‡ä»¶å: {os.path.basename(saved_path)}")
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
        try:
            with open(saved_path, 'r', encoding='utf-8') as f:
                saved_data = json.load(f)
            print(f"  ğŸ“Š ä¿å­˜çš„æ•°æ®åŒ…å«:")
            print(f"    å…ƒæ•°æ®: âœ…")
            print(f"    è¾“å…¥æ•°æ®æ‘˜è¦: âœ…")
            print(f"    é¢„æµ‹ç»“æœ: {len(saved_data.get('prediction_results', []))} æ¡")
            print(f"    å®é™…æ•°æ®: {len(saved_data.get('actual_data', []))} æ¡")
            print(f"    åˆ†æç»“æœ: {'âœ…' if saved_data.get('analysis') else 'âŒ'}")
        except Exception as e:
            print(f"  âš ï¸  éªŒè¯ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    else:
        print(f"  âŒ é¢„æµ‹ç»“æœä¿å­˜å¤±è´¥")


if __name__ == '__main__':
    """
    ä¸»ç¨‹åºå…¥å£
    è¿è¡Œæ‰€æœ‰æµ‹è¯•å‡½æ•°ä»¥éªŒè¯æ¨¡å—åŠŸèƒ½
    """
    print("ğŸš€ å¯åŠ¨ Kronos æ¨¡å‹ç®¡ç†æ¨¡å—æµ‹è¯•")
    print(f"ğŸ“¦ æ¨¡å‹å¯ç”¨æ€§: {'âœ… å¯ç”¨' if MODEL_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    
    if not MODEL_AVAILABLE:
        print("âš ï¸  è­¦å‘Š: Kronosæ¨¡å‹æ¨¡å—ä¸å¯ç”¨")
        print("ğŸ’¡ è¿™å¯èƒ½æ˜¯å› ä¸º:")
        print("   1. æ¨¡å‹æ¨¡å—æœªæ­£ç¡®å®‰è£…")
        print("   2. Pythonè·¯å¾„é…ç½®é—®é¢˜")
        print("   3. ä¾èµ–åº“ç¼ºå¤±")
        print("\nğŸ”„ å°†ç»§ç»­è¿è¡Œå…¶ä»–åŠŸèƒ½çš„æµ‹è¯•...")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    try:
        test_data_loading()
        test_model_functions()
        test_utility_functions()
        
        print("\n" + "="*50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("="*50)
        print("âœ… æ•°æ®åŠ è½½åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        print("âœ… æ¨¡å‹åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        print("âœ… å·¥å…·å‡½æ•°æµ‹è¯•å®Œæˆ")
        print("\nğŸ’¡ å¦‚éœ€å¯åŠ¨Flask WebæœåŠ¡ï¼Œè¯·è¿è¡Œ app.py")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("\nğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ç›¸å…³é—®é¢˜")
